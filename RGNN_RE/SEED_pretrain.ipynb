{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import hdf5storage as hdf5\n",
    "import einops\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_normal(sample):\n",
    "    for i in range(len(sample)):\n",
    "        features_min = np.min(sample[i])\n",
    "        features_max = np.max(sample[i])\n",
    "        sample[i] = (sample[i] - features_min) / (features_max - features_min)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  0 -1 -1  0  1 -1  0  1  1  0 -1  0  1 -1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0], dtype=int16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_path = \"D:\\SEED\\ExtractedFeatures\"\n",
    "data_path = \"data\\seed\" \n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "label = hdf5.loadmat(os.path.join(raw_data_path, \"label.mat\"))['label'][0]\n",
    "print(label)\n",
    "label += np.ones(label.size, dtype=np.int16)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress on features: 100%|█████████████████████████████████████████| 45/45 [00:22<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_extracted_features_dataset(folder_path, feature_name, win, frequency_band=None):\n",
    "    '''\n",
    "        将 folder_path 文件夹中的 ExtractedFeatures 数据转化为机器学习常用的数据集，区分开不同 trial 的数据\n",
    "        ToDo: 增加 channel 的选择，而不是使用所有的 channel\n",
    "    :param folder_path: ExtractedFeatures 文件夹对应的路径\n",
    "    :param feature_name: 需要使用的特征名，如 'de_LDS'，'asm_LDS' 等，以 de_LDS1 为例，维度为 62 * 235 * 5，235为影片长度235秒，每秒切分为一个样本，62为通道数，5为频带数\n",
    "    :param frequency_band: 需要选取的频带，'delta', 'theta', 'alpha', 'beta', 'gamma'\n",
    "    :return feature_vector_dict, label_dict: 分别为样本的特征向量，样本的标签，key 为被试名字，val 为该被试对应的特征向量或标签的 list，方便 subject-independent 的测试\n",
    "    '''\n",
    "    band_map = {'delta': 0, 'theta': 1, 'alpha': 2, 'beta': 3, 'gamma': 4} # 频带映射:deta->0,theta->1,alpha->2,beta->3,gamma->4\n",
    "    label_path = os.path.join(folder_path, 'label.mat')\n",
    "    labels = hdf5.loadmat(label_path, verify_compressed_data_integrity=False)['label'][0] # [1, 0, -1, -1, 0, 1, -1, 0, 1, 1, 0, -1, 0, 1, -1]\n",
    "    labels += np.ones(label.size, dtype=np.int16) # [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0]\n",
    "    feature_vector_dict = {}\n",
    "    label_dict = {}\n",
    "    try:\n",
    "        file_list = [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file)) and re.match(r'\\d+_\\d+.mat', file)]\n",
    "        file_list = sorted(file_list, key=lambda x: int(x.split('_')[0]))\n",
    "        for file_name in tqdm(file_list, desc=f'Progress on features', ncols=100): # 45个mat\n",
    "            # print(f'Now, processing {file_name}')\n",
    "            all_features_dict = hdf5.loadmat(os.path.join(folder_path, file_name), verify_compressed_data_integrity=False) # 包含15个trial的各种特征\n",
    "            subject_name = file_name.split('.')[0] # 被试如10_20131130\n",
    "            feature_vector_trial_list = []\n",
    "            label_trial_list = []\n",
    "            for trial in range(1, 16):\n",
    "                # cur_feature = all_features_dict[feature_name + str(trial)][:,:180,:] # 取出一共trial的特征，如de_LDS1：62*235*5\n",
    "                cur_feature = all_features_dict[feature_name + str(trial)] # 取出一共trial的特征，如de_LDS1：62*235*5\n",
    "                if frequency_band:\n",
    "                    frequency_idx = band_map[frequency_band] \n",
    "                    cur_feature = np.asarray(cur_feature[:, :, frequency_idx]).T  # 取出特定频段的数据，62*235，转置后，维度为 N * 62, N 为影片长度（235s）\n",
    "                else:\n",
    "                    cur_feature = np.asarray(cur_feature.transpose(1,0,2)) # 62*235*5 -> 235*62*5\n",
    "                    # num_win_trial = cur_feature.shape[1] # 235\n",
    "                    # trial_data = []\n",
    "                    # for j in range(0, num_win_trial, win): # concat w_len samples\n",
    "                    #     if (j + win > num_win_trial): break\n",
    "                    #     window = cur_feature[:, j:j+win, :]\n",
    "                    #     assert (window.shape == (62, win, 5))\n",
    "                    #     window = np.reshape(window, (62, -1))\n",
    "                    #     assert (window.shape == (62, win*5))\n",
    "                    #     trial_data.append(window)\n",
    "                    # cur_feature = np.stack(trial_data, axis=0) # 47*62*25\n",
    "                cur_label = np.asarray([labels[trial - 1]] * cur_feature.shape[0]) # 生成标签，长度为235\n",
    "                feature_vector_trial_list.append(cur_feature)\n",
    "                label_trial_list.append(cur_label)\n",
    "                assert cur_feature.shape[0] == cur_label.shape[0], f'Feature and label shape mismatch, feature shape: {cur_feature.shape}, label shape: {cur_label.shape}'\n",
    "                assert (cur_label[0] == labels[trial-1])            \n",
    "            feature_vector_trial = np.concatenate(feature_vector_trial_list, axis=0)\n",
    "            label_trial = np.concatenate(label_trial_list, axis=0)\n",
    "            assert feature_vector_trial.shape[0] == label_trial.shape[0], f'Feature and label shape mismatch, feature shape: {feature_vector_trial.shape}, label shape: {label_trial.shape}'\n",
    "            \n",
    "            feature_vector_dict[subject_name] = feature_vector_trial\n",
    "            label_dict[subject_name] = label_trial\n",
    "    except FileNotFoundError as e:\n",
    "        print('加载数据时出错: {}'.format(e))\n",
    "    return feature_vector_dict, label_dict\n",
    "de_vector_dict, label_dict = build_extracted_features_dataset(raw_data_path, 'de_LDS', win=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_20131027 (3394, 62, 5) (3394,)\n",
      "1_20131030 (3394, 62, 5) (3394,)\n",
      "1_20131107 (3394, 62, 5) (3394,)\n",
      "2_20140404 (3394, 62, 5) (3394,)\n",
      "2_20140413 (3394, 62, 5) (3394,)\n",
      "2_20140419 (3394, 62, 5) (3394,)\n",
      "3_20140603 (3394, 62, 5) (3394,)\n",
      "3_20140611 (3394, 62, 5) (3394,)\n",
      "3_20140629 (3394, 62, 5) (3394,)\n",
      "4_20140621 (3394, 62, 5) (3394,)\n",
      "4_20140702 (3394, 62, 5) (3394,)\n",
      "4_20140705 (3394, 62, 5) (3394,)\n",
      "5_20140411 (3394, 62, 5) (3394,)\n",
      "5_20140418 (3394, 62, 5) (3394,)\n",
      "5_20140506 (3394, 62, 5) (3394,)\n",
      "6_20130712 (3394, 62, 5) (3394,)\n",
      "6_20131016 (3394, 62, 5) (3394,)\n",
      "6_20131113 (3394, 62, 5) (3394,)\n",
      "7_20131027 (3394, 62, 5) (3394,)\n",
      "7_20131030 (3394, 62, 5) (3394,)\n",
      "7_20131106 (3394, 62, 5) (3394,)\n",
      "8_20140511 (3394, 62, 5) (3394,)\n",
      "8_20140514 (3394, 62, 5) (3394,)\n",
      "8_20140521 (3394, 62, 5) (3394,)\n",
      "9_20140620 (3394, 62, 5) (3394,)\n",
      "9_20140627 (3394, 62, 5) (3394,)\n",
      "9_20140704 (3394, 62, 5) (3394,)\n",
      "10_20131130 (3394, 62, 5) (3394,)\n",
      "10_20131204 (3394, 62, 5) (3394,)\n",
      "10_20131211 (3394, 62, 5) (3394,)\n",
      "11_20140618 (3394, 62, 5) (3394,)\n",
      "11_20140625 (3394, 62, 5) (3394,)\n",
      "11_20140630 (3394, 62, 5) (3394,)\n",
      "12_20131127 (3394, 62, 5) (3394,)\n",
      "12_20131201 (3394, 62, 5) (3394,)\n",
      "12_20131207 (3394, 62, 5) (3394,)\n",
      "13_20140527 (3394, 62, 5) (3394,)\n",
      "13_20140603 (3394, 62, 5) (3394,)\n",
      "13_20140610 (3394, 62, 5) (3394,)\n",
      "14_20140601 (3394, 62, 5) (3394,)\n",
      "14_20140615 (3394, 62, 5) (3394,)\n",
      "14_20140627 (3394, 62, 5) (3394,)\n",
      "15_20130709 (3394, 62, 5) (3394,)\n",
      "15_20131016 (3394, 62, 5) (3394,)\n",
      "15_20131105 (3394, 62, 5) (3394,)\n"
     ]
    }
   ],
   "source": [
    "for key in de_vector_dict.keys():\n",
    "    print(key, de_vector_dict[key].shape, label_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress on DE features: 100%|██████████████████████████████████████| 15/15 [00:02<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sub_1', 'sub_2', 'sub_3', 'sub_4', 'sub_5', 'sub_6', 'sub_7', 'sub_8', 'sub_9', 'sub_10', 'sub_11', 'sub_12', 'sub_13', 'sub_14', 'sub_15'])\n",
      "sub_1 (10182, 62, 5) (10182,)\n",
      "sub_2 (10182, 62, 5) (10182,)\n",
      "sub_3 (10182, 62, 5) (10182,)\n",
      "sub_4 (10182, 62, 5) (10182,)\n",
      "sub_5 (10182, 62, 5) (10182,)\n",
      "sub_6 (10182, 62, 5) (10182,)\n",
      "sub_7 (10182, 62, 5) (10182,)\n",
      "sub_8 (10182, 62, 5) (10182,)\n",
      "sub_9 (10182, 62, 5) (10182,)\n",
      "sub_10 (10182, 62, 5) (10182,)\n",
      "sub_11 (10182, 62, 5) (10182,)\n",
      "sub_12 (10182, 62, 5) (10182,)\n",
      "sub_13 (10182, 62, 5) (10182,)\n",
      "sub_14 (10182, 62, 5) (10182,)\n",
      "sub_15 (10182, 62, 5) (10182,)\n"
     ]
    }
   ],
   "source": [
    "def split_with_sub(data_dic, label_dic, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    assert data_dic.keys() == label_dic.keys(), 'Data and label keys mismatch'\n",
    "    \n",
    "    data_sub_dic = {}\n",
    "    label_sub_dic = {}\n",
    "\n",
    "    for sub_id in tqdm(range(1,16), desc=f'Progress on DE features', ncols=100):\n",
    "        feature_list = [] # M * 62 * 5\n",
    "        label_list = [] # M,\n",
    "        for experiment_name in data_dic.keys():\n",
    "            if int(experiment_name.split('_')[0]) != sub_id: continue\n",
    "            # print(f'Current processing: {experiment_name}')\n",
    "            feature_list.append(data_dic[experiment_name])\n",
    "            label_list.append(label_dic[experiment_name])\n",
    "        assert(len(feature_list) == 3), 'Feature list length mismatch'\n",
    "        feature_array = extend_normal(np.concatenate(feature_list, axis=0))\n",
    "        label_array = np.concatenate(label_list, axis=0)\n",
    "        data_sub_dic[f'sub_{sub_id}'] = feature_array\n",
    "        label_sub_dic[f'sub_{sub_id}'] = label_array\n",
    "    # dic保存为mat文件\n",
    "    sio.savemat(os.path.join(path,'data_dic.mat'), data_sub_dic)\n",
    "    sio.savemat(os.path.join(path,'label_dic.mat'), label_sub_dic)\n",
    "    return data_sub_dic, label_sub_dic\n",
    "\n",
    "data_sub_dic, label_sub_dic = split_with_sub(de_vector_dict, label_dict, data_path)\n",
    "print(data_sub_dic.keys())\n",
    "for key in data_sub_dic.keys():\n",
    "    print(key, data_sub_dic[key].shape, label_sub_dic[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_1 (10182, 62, 5) (1, 10182)\n",
      "sub_2 (10182, 62, 5) (1, 10182)\n",
      "sub_3 (10182, 62, 5) (1, 10182)\n",
      "sub_4 (10182, 62, 5) (1, 10182)\n",
      "sub_5 (10182, 62, 5) (1, 10182)\n",
      "sub_6 (10182, 62, 5) (1, 10182)\n",
      "sub_7 (10182, 62, 5) (1, 10182)\n",
      "sub_8 (10182, 62, 5) (1, 10182)\n",
      "sub_9 (10182, 62, 5) (1, 10182)\n",
      "sub_10 (10182, 62, 5) (1, 10182)\n",
      "sub_11 (10182, 62, 5) (1, 10182)\n",
      "sub_12 (10182, 62, 5) (1, 10182)\n",
      "sub_13 (10182, 62, 5) (1, 10182)\n",
      "sub_14 (10182, 62, 5) (1, 10182)\n",
      "sub_15 (10182, 62, 5) (1, 10182)\n"
     ]
    }
   ],
   "source": [
    "save_path = \"data\\seed\" \n",
    "data_dic = hdf5.loadmat(os.path.join(save_path, 'data_dic.mat'))\n",
    "label_dic = hdf5.loadmat(os.path.join(save_path, 'label_dic.mat'))\n",
    "for key in data_dic.keys():\n",
    "    if key.endswith('__'): continue\n",
    "    print(key, data_dic[key].shape, label_dic[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_dic.keys()) == set(label_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(label_dic[key][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA TITAN Xp', major=6, minor=1, total_memory=12287MB, multi_processor_count=30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
