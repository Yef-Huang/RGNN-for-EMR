{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "import pickle\n",
    "import hdf5storage as hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\SEED\\\\ExtractedFeatures'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = 'D:\\SEED'\n",
    "feature_path = os.path.join(root_path,'ExtractedFeatures')\n",
    "feature_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling frequency\n",
    "sf = 200\n",
    "\n",
    "# hyper-parameters\n",
    "num_trials = 15\n",
    "num_subjects = 15\n",
    "num_bands = 5\n",
    "num_classes = 3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample shape: (num_channels, num_windows, num_bands)\n",
      "(62, 235, 5)\n",
      "Feature names LDS averaged\n",
      "['de_LDS1', 'psd_LDS1', 'dasm_LDS1', 'rasm_LDS1', 'asm_LDS1', 'dcau_LDS1', 'de_LDS2', 'psd_LDS2', 'dasm_LDS2', 'rasm_LDS2', 'asm_LDS2', 'dcau_LDS2', 'de_LDS3', 'psd_LDS3', 'dasm_LDS3', 'rasm_LDS3', 'asm_LDS3', 'dcau_LDS3', 'de_LDS4', 'psd_LDS4', 'dasm_LDS4', 'rasm_LDS4', 'asm_LDS4', 'dcau_LDS4', 'de_LDS5', 'psd_LDS5', 'dasm_LDS5', 'rasm_LDS5', 'asm_LDS5', 'dcau_LDS5', 'de_LDS6', 'psd_LDS6', 'dasm_LDS6', 'rasm_LDS6', 'asm_LDS6', 'dcau_LDS6', 'de_LDS7', 'psd_LDS7', 'dasm_LDS7', 'rasm_LDS7', 'asm_LDS7', 'dcau_LDS7', 'de_LDS8', 'psd_LDS8', 'dasm_LDS8', 'rasm_LDS8', 'asm_LDS8', 'dcau_LDS8', 'de_LDS9', 'psd_LDS9', 'dasm_LDS9', 'rasm_LDS9', 'asm_LDS9', 'dcau_LDS9', 'de_LDS10', 'psd_LDS10', 'dasm_LDS10', 'rasm_LDS10', 'asm_LDS10', 'dcau_LDS10', 'de_LDS11', 'psd_LDS11', 'dasm_LDS11', 'rasm_LDS11', 'asm_LDS11', 'dcau_LDS11', 'de_LDS12', 'psd_LDS12', 'dasm_LDS12', 'rasm_LDS12', 'asm_LDS12', 'dcau_LDS12', 'de_LDS13', 'psd_LDS13', 'dasm_LDS13', 'rasm_LDS13', 'asm_LDS13', 'dcau_LDS13', 'de_LDS14', 'psd_LDS14', 'dasm_LDS14', 'rasm_LDS14', 'asm_LDS14', 'dcau_LDS14', 'de_LDS15', 'psd_LDS15', 'dasm_LDS15', 'rasm_LDS15', 'asm_LDS15', 'dcau_LDS15']\n",
      "<class 'numpy.int16'>\n",
      "[2 1 0 0 1 2 0 1 2 2 1 0 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# get only subject data and sort for convenience\n",
    "data = os.listdir(feature_path)\n",
    "# only take subject files\n",
    "data = [x for x in data if len(x.split(\"_\")) == 2] \n",
    "data.sort(key = lambda x : int(x.split(\"_\")[0]))\n",
    "# 3 files per subject, each file contains recordings for 15 trials\n",
    "assert (len(data) == 45)\n",
    "\n",
    "# load one sample \n",
    "sample = hdf5.loadmat(os.path.join(feature_path, data[0]))\n",
    "\n",
    "keys = list(sample.keys())\n",
    "assert (len(keys) == (2*6*15+3)) # 3 meta keys\n",
    "print(\"One sample shape: (num_channels, num_windows, num_bands)\")\n",
    "print(sample[\"de_LDS1\"].shape)\n",
    "\n",
    "# get all features averaged with LDS\n",
    "features_LDS = keys[4::2]\n",
    "print(\"Feature names LDS averaged\")\n",
    "print(features_LDS)\n",
    "assert (len(features_LDS) == (15*6))\n",
    "\n",
    "labels = hdf5.loadmat(os.path.join(feature_path, \"label.mat\"))\n",
    "labels = np.squeeze(labels[\"label\"] + np.ones(15, dtype=np.int8))\n",
    "assert (labels.shape == (15,))\n",
    "print(type(labels[0]))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-23.825413747036478, 1072646499245.6045)\n",
      "de range: (10.567626836074302, 42.11366999020901)\n"
     ]
    }
   ],
   "source": [
    "# get the range of values across samples from de_LDS feature\n",
    "max_value = -1e18\n",
    "min_value = 1e18\n",
    "de_max_value = -1e18\n",
    "de_min_value = 1e18\n",
    "\n",
    "sample = hdf5.loadmat(os.path.join(feature_path, data[0]))\n",
    "# get all the de_lds feature keys since for the final model I will only use de_lds features\n",
    "de_keys = [key for key in sample.keys() if \"de_LDS\" in key]\n",
    "assert (len(de_keys) == 15)\n",
    "\n",
    "for sample in data:\n",
    "  sample = hdf5.loadmat(os.path.join(feature_path, sample))\n",
    "  for key in features_LDS:\n",
    "    if key in de_keys:\n",
    "      de_max_value = max(de_max_value, np.amax(sample[key]))\n",
    "      de_min_value = min(de_min_value, np.amin(sample[key]))\n",
    "    max_value = max(max_value, np.amax(sample[key]))\n",
    "    min_value = min(min_value, np.amin(sample[key]))\n",
    "print((min_value, max_value))\n",
    "print(f'de range: {(de_min_value, de_max_value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate more windows in one training sample\n",
    "*The above blocks treat a data sample as a single 1s window, however in order to effectively classify the input more time steps (1s windows) need to be taken into consideration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wind_from_file(file:str, w_len, drop_incomplete=True):\n",
    "  x = hdf5.loadmat(os.path.join(feature_path, file))\n",
    "  all_data, all_labels = [], []\n",
    "  total_num_wind = 0\n",
    "\n",
    "  for i in range(0,90,6):\n",
    "    trial = int(i//6) #0-indexed\n",
    "    trial_data = []\n",
    "\n",
    "    # take only de_lds feature\n",
    "    assert (features_LDS[i] == f\"de_LDS{trial+1}\")\n",
    "    f = x[features_LDS[i]] # 62*235*5\n",
    "    num_wind_trial = f.shape[1]\n",
    "    \n",
    "    for j in range(0, num_wind_trial, w_len): # concat w_len samples\n",
    "      if drop_incomplete is True and (j + w_len > num_wind_trial): break;\n",
    "      window = f[:, j:j+w_len, :]\n",
    "      assert (window.shape == (62, w_len, 5))\n",
    "      window = np.reshape(window, (62, -1))\n",
    "      assert (window.shape == (62, w_len*5))\n",
    "      trial_data.append(window)\n",
    "    \n",
    "    trial_data = np.stack(trial_data, axis=0) # 47*62*25\n",
    "    num_wind_trial = trial_data.shape[0] # 47,\n",
    "    total_num_wind += num_wind_trial\n",
    "\n",
    "    assert (trial_data.shape == (num_wind_trial, 62, w_len*num_bands))\n",
    "    assert (np.amax(trial_data) <= de_max_value)\n",
    "    assert (np.amin(trial_data) >= de_min_value)\n",
    "\n",
    "    # assign to each window the corresponding trial label\n",
    "    trial_labels = np.array(list([labels[trial]] * trial_data.shape[0])) # 47,\n",
    "    assert (np.unique(trial_labels).shape == (1,))\n",
    "    assert (trial_labels[0] == labels[trial])\n",
    "\n",
    "    all_data.append(trial_data)\n",
    "    all_labels.append(trial_labels)\n",
    "    \n",
    "  all_data = np.concatenate(all_data, axis=0)\n",
    "  all_labels = np.concatenate(all_labels, axis=0)\n",
    "  assert (all_data.shape == (total_num_wind, 62, num_bands*w_len))\n",
    "  assert (all_labels.shape == (total_num_wind,))\n",
    "  assert (np.amax(all_data) <= de_max_value)\n",
    "  assert (np.amin(all_data) >= de_min_value)\n",
    "  return all_data, all_labels\n",
    "\n",
    "def get_wind_for_subject(file1, file2, file3, w_len):\n",
    "  series1, l1 = get_wind_from_file(file1, w_len) # 675*62*25; 675,\n",
    "  series2, l2 = get_wind_from_file(file2, w_len) # 675*62*25; 675,\n",
    "  series3, l3 = get_wind_from_file(file3, w_len) # 675*62*25; 675,\n",
    "\n",
    "  series = np.concatenate([series1, series2, series3], axis=0) # 2025*62*25\n",
    "  l = np.concatenate([l1, l2, l3], axis=0) # 2025,\n",
    "  assert (series.shape == (3*series1.shape[0], 62, w_len*num_bands))\n",
    "  assert (l.shape == (3*series1.shape[0],))\n",
    "  assert (np.amax(series) <= de_max_value)\n",
    "  assert (np.amin(series) >= de_min_value)\n",
    "  return series, l\n",
    "\n",
    "# def get_more_windows_data(w_len):\n",
    "w_len = 5\n",
    "all_data, all_labels = [], []\n",
    "dic_all_data = {}\n",
    "dic_all_labels = {}\n",
    "for i in range(0,45,3):\n",
    "  subject_data, subject_labels = get_wind_for_subject(data[i], data[i+1], data[i+2], w_len)\n",
    "  # with open(NPY_PATH + \"/\" + \"npy_\" + str((i+1)//3) + \".npy\", 'wb') as f:\n",
    "  #   np.save(f, subject_data)\n",
    "  # with open(NPY_PATH + \"/\" + \"npy_\" + str((i+1)//3) + \"_label.npy\", 'wb') as f:\n",
    "  #   np.save(f, subject_labels)\n",
    "  dic_all_data[f'sub_{((i+1)//3)+1}'] = subject_data\n",
    "  dic_all_labels[f'sub_{((i+1)//3)+1}'] = subject_labels\n",
    "  all_data.append(subject_data)\n",
    "  all_labels.append(subject_labels)\n",
    "  \n",
    "all_data = np.concatenate(all_data, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "assert (all_data.shape[0] == all_labels.shape[0])\n",
    "# with open(NPY_PATH + \"/\" + \"npy_all_subjects.npy\", 'wb') as f:\n",
    "#     np.save(f, all_data)\n",
    "# with open(NPY_PATH + \"/\" + \"npy_all_subjects_label.npy\", 'wb') as f:\n",
    "#     np.save(f, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_1 (2025, 62, 25) (2025,)\n",
      "sub_2 (2025, 62, 25) (2025,)\n",
      "sub_3 (2025, 62, 25) (2025,)\n",
      "sub_4 (2025, 62, 25) (2025,)\n",
      "sub_5 (2025, 62, 25) (2025,)\n",
      "sub_6 (2025, 62, 25) (2025,)\n",
      "sub_7 (2025, 62, 25) (2025,)\n",
      "sub_8 (2025, 62, 25) (2025,)\n",
      "sub_9 (2025, 62, 25) (2025,)\n",
      "sub_10 (2025, 62, 25) (2025,)\n",
      "sub_11 (2025, 62, 25) (2025,)\n",
      "sub_12 (2025, 62, 25) (2025,)\n",
      "sub_13 (2025, 62, 25) (2025,)\n",
      "sub_14 (2025, 62, 25) (2025,)\n",
      "sub_15 (2025, 62, 25) (2025,)\n",
      "total: (30375, 62, 25) (30375,)\n"
     ]
    }
   ],
   "source": [
    "for key in dic_all_data.keys():\n",
    "  print(key, dic_all_data[key].shape, dic_all_labels[key].shape)\n",
    "print('total:', all_data.shape, all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sub_1', 'sub_2', 'sub_3', 'sub_4', 'sub_5', 'sub_6', 'sub_7', 'sub_8', 'sub_9', 'sub_10', 'sub_11', 'sub_12', 'sub_13', 'sub_14', 'sub_15'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'seed_processed'\n",
    "if os.path.exists(save_path) is False:\n",
    "  os.makedirs(save_path)\n",
    "with open(save_path + \"/\" + \"npy_all_subjects.npy\", 'wb') as f:\n",
    "    np.save(f, all_data)\n",
    "with open(save_path + \"/\" + \"npy_all_subjects_label.npy\", 'wb') as f:\n",
    "    np.save(f, all_labels)\n",
    "# dic保存为mat文件\n",
    "io.savemat(os.path.join(save_path,'dic_all_subjects.mat'), dic_all_data)\n",
    "io.savemat(os.path.join(save_path,'dic_all_subjects_label.mat'), dic_all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_1 (2025, 62, 25) (1, 2025)\n",
      "sub_2 (2025, 62, 25) (1, 2025)\n",
      "sub_3 (2025, 62, 25) (1, 2025)\n",
      "sub_4 (2025, 62, 25) (1, 2025)\n",
      "sub_5 (2025, 62, 25) (1, 2025)\n",
      "sub_6 (2025, 62, 25) (1, 2025)\n",
      "sub_7 (2025, 62, 25) (1, 2025)\n",
      "sub_8 (2025, 62, 25) (1, 2025)\n",
      "sub_9 (2025, 62, 25) (1, 2025)\n",
      "sub_10 (2025, 62, 25) (1, 2025)\n",
      "sub_11 (2025, 62, 25) (1, 2025)\n",
      "sub_12 (2025, 62, 25) (1, 2025)\n",
      "sub_13 (2025, 62, 25) (1, 2025)\n",
      "sub_14 (2025, 62, 25) (1, 2025)\n",
      "sub_15 (2025, 62, 25) (1, 2025)\n"
     ]
    }
   ],
   "source": [
    "data_dic = io.loadmat(os.path.join(save_path, 'dic_all_subjects.mat'))\n",
    "label_dic = io.loadmat(os.path.join(save_path, 'dic_all_subjects_label.mat'))\n",
    "for key in data_dic.keys():\n",
    "    if key.endswith('__'): continue\n",
    "    print(key, data_dic[key].shape, label_dic[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 0, 0, 0], dtype=int16)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic[key][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch18py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
